# Velocity Field Divergence Computation

**Date**: December 16, 2025

## Overview

Added functions to compute the divergence of the velocity field, correctly accounting for what quantity the CRN is predicting (velocity, noise, or target).

## Mathematical Background

### Flow Matching
```
x_t = (1-t)Â·x_0 + tÂ·x_1
v(x_t, t) = dx/dt = x_1 - x_0
```

### Divergence
For a vector field `v: R^D -> R^D`, the divergence is:
```
div(v) = âˆ‡Â·v = sum_i âˆ‚v_i/âˆ‚x_i = trace(J_v)
```

where `J_v` is the Jacobian matrix of `v` with respect to `x`.

## Key Insight

The velocity field `v` is derived from the CRN output, and the relationship depends on what the CRN predicts. This affects the divergence computation!

### Case 1: CRN Predicts Velocity

```
v = crn_output
div(v) = trace(J_crn)
```

Simple! The divergence is just the trace of the CRN's Jacobian.

### Case 2: CRN Predicts Noise (Îµ = x_1)

```
v = (Îµ - x_t) / (1-t)
```

Taking the divergence:
```
âˆ‚v_i/âˆ‚x_i = (âˆ‚Îµ_i/âˆ‚x_i - 1) / (1-t)
div(v) = sum_i âˆ‚v_i/âˆ‚x_i
       = (sum_i âˆ‚Îµ_i/âˆ‚x_i - D) / (1-t)
       = (trace(J_crn) - D) / (1-t)
```

### Case 3: CRN Predicts Target (x_0)

```
v = (x_t - x_0) / t
```

Taking the divergence:
```
âˆ‚v_i/âˆ‚x_i = (1 - âˆ‚x0_i/âˆ‚x_i) / t
div(v) = sum_i âˆ‚v_i/âˆ‚x_i
       = (D - sum_i âˆ‚x0_i/âˆ‚x_i) / t
       = (D - trace(J_crn)) / t
```

## Implementation

### Function 1: Compute CRN Jacobian Trace

```python
def compute_crn_jacobian_trace(self, x_t, z, t, mask=None, hutchinson_samples=1):
    """
    Compute trace of Jacobian of CRN output w.r.t. x_t using Hutchinson's estimator.
    
    Hutchinson's trace estimator: trace(J) â‰ˆ E[v^T J v] where v ~ Rademacher(Â±1)
    
    This is much more efficient than computing the full Jacobian:
    - Full Jacobian: O(DÂ²) time and memory per point
    - Hutchinson: O(D) time and memory per point
    
    Args:
        x_t: Input points, shape (B, N, D)
        z: Latent context, shape (B, Dc) or (B, K, Dc)
        t: Time, scalar or (B,) or (B, 1)
        mask: Optional mask, shape (B, N)
        hutchinson_samples: Number of random samples (1 is usually sufficient)
    
    Returns:
        Trace of Jacobian for each point, shape (B, N)
    """
    # Uses Hutchinson's trace estimator
    # Computes Jacobian-vector products using jax.jvp
    # Vectorized over batch and points using vmap
```

**Implementation Details:**
- Uses **Hutchinson's trace estimator**: `trace(J) â‰ˆ E[v^T J v]`
- Samples random vectors `v` from Rademacher distribution (Â±1)
- Computes Jacobian-vector products using `jax.jvp` (forward-mode AD)
- **O(D) complexity** instead of O(DÂ²) for full Jacobian
- Stochastic but unbiased estimator
- Standard practice in continuous normalizing flows (CNFs)

### Function 2: Compute Velocity Divergence

```python
def compute_velocity_divergence(self, x_t, z, t, mask=None):
    """
    Compute divergence of the velocity field.
    
    The velocity field is v(x_t, t) and its divergence depends on 
    what the CRN predicts.
    
    Args:
        x_t: Input points, shape (B, N, D)
        z: Latent context, shape (B, Dc) or (B, K, Dc)
        t: Time, scalar or (B,) or (B, 1)
        mask: Optional mask, shape (B, N)
    
    Returns:
        Divergence of velocity field, shape (B, N)
    """
    # Computes trace(J_crn)
    # Applies appropriate formula based on prediction_target
```

**Formulas Used:**
```python
if prediction_target == VELOCITY:
    div_v = trace_jac_crn

elif prediction_target == NOISE:
    div_v = (trace_jac_crn - D) / (1 - t)

elif prediction_target == TARGET:
    div_v = (D - trace_jac_crn) / t
```

## Usage

### Basic Usage

```python
from src.models.mnist_flow_2d import MnistFlow2D, PredictionTarget

# Create model
model = MnistFlow2D(
    latent_dim=64,
    encoder_type='pointnet',
    prediction_target=PredictionTarget.VELOCITY
)

# Initialize
params = model.init(key, x, key)

# Encode to get context
z, mu, logvar = model.apply(params, x, key, method=model.encode)

# Compute divergence at time t
t = 0.5
x_t = ...  # Interpolated points
div_v = model.apply(
    params, x_t, z, t,
    method=model.compute_velocity_divergence
)
# div_v shape: (B, N)
```

### With Different Prediction Targets

```python
# Velocity prediction
model_v = MnistFlow2D(prediction_target=PredictionTarget.VELOCITY)
div_v = model_v.apply(params, x_t, z, t, method=model_v.compute_velocity_divergence)
# div_v = trace(J_crn)

# Noise prediction
model_n = MnistFlow2D(prediction_target=PredictionTarget.NOISE)
div_n = model_n.apply(params, x_t, z, t, method=model_n.compute_velocity_divergence)
# div_n = (trace(J_crn) - D) / (1-t)

# Target prediction
model_t = MnistFlow2D(prediction_target=PredictionTarget.TARGET)
div_t = model_t.apply(params, x_t, z, t, method=model_t.compute_velocity_divergence)
# div_t = (D - trace(J_crn)) / t
```

### Computing Just the Jacobian Trace

```python
# If you only need the CRN Jacobian trace (default: 1 sample)
trace_jac = model.apply(
    params, x_t, z, t,
    method=model.compute_crn_jacobian_trace
)
# trace_jac shape: (B, N)

# Use more samples to reduce variance (optional)
trace_jac = model.apply(
    params, x_t, z, t,
    hutchinson_samples=5,
    method=model.compute_crn_jacobian_trace
)
```

## Use Cases

### 1. Continuous Normalizing Flows (CNFs)

The divergence is needed for computing the change in log-probability:
```python
# Log-probability change
d_log_p = -div_v  # (B, N)

# Integrate along trajectory
log_p_t = log_p_0 - integral(div_v dt)
```

### 2. Probability Path Computation

For flow matching with exact likelihood:
```python
# At each time step
div_v_t = model.compute_velocity_divergence(x_t, z, t)
log_det_change = -jnp.sum(div_v_t, axis=-1)  # (B,)
```

### 3. Regularization

Penalize high divergence for smoother flows:
```python
# Divergence regularization
div_v = model.compute_velocity_divergence(x_t, z, t)
div_penalty = jnp.mean(div_v ** 2)
loss = reconstruction_loss + lambda_div * div_penalty
```

## Performance Notes

### Computational Cost

**Hutchinson's Estimator (Current Implementation):**
- **Jacobian-vector product**: `O(D)` per point per sample
- **Total**: `O(B * N * D * S)` for batch, where S = hutchinson_samples
- **Default S=1**: `O(B * N * D)` - linear in dimension!

**Full Jacobian (Old Approach):**
- **Jacobian computation**: `O(DÂ²)` per point
- **Total**: `O(B * N * DÂ²)` for batch

**Speedup**: For D-dimensional space, Hutchinson is **D times faster** than full Jacobian!

For 2D points (D=2): Both methods are fast (2x speedup)
For 3D points (D=3): Hutchinson is 3x faster
For 100D: Hutchinson is 100x faster!

### Memory

**Hutchinson's Estimator:**
- Jacobian-vector product: `O(D)` per point
- No need to store full Jacobian matrix

**Full Jacobian:**
- Full Jacobian matrix: `O(DÂ²)` per point
- Much higher memory usage for large D

### Variance Control

Hutchinson's estimator is stochastic but unbiased:
- **1 sample** (default): Fast, usually sufficient
- **5-10 samples**: Lower variance, slightly slower
- **Variance decreases** as `1/âˆšS` where S = number of samples

```python
# Trade-off between speed and variance
trace_1 = compute_crn_jacobian_trace(x, z, t, hutchinson_samples=1)   # Fast
trace_5 = compute_crn_jacobian_trace(x, z, t, hutchinson_samples=5)   # More stable
trace_10 = compute_crn_jacobian_trace(x, z, t, hutchinson_samples=10) # Very stable
```

### Why Hutchinson Works

The key insight: **trace(J) = E[v^T J v]** for random v ~ Rademacher(Â±1)

Proof:
```
E[v^T J v] = E[sum_i sum_j v_i J_ij v_j]
           = sum_i sum_j J_ij E[v_i v_j]
           = sum_i sum_j J_ij Î´_ij    (E[v_i v_j] = 1 if i=j, 0 otherwise)
           = sum_i J_ii
           = trace(J)
```

This allows us to estimate the trace using only **Jacobian-vector products** (cheap!)
instead of computing the full Jacobian matrix (expensive!).

## Test Results

### Hutchinson's Estimator Tests

```
âœ… Test 1: Basic functionality with Hutchinson estimator
   Trace shape: (2, 10)
   Trace mean: 0.0000 âœ“

âœ… Test 2: Divergence computation with different prediction targets
   velocity  : div shape=(2, 10), mean=0.0000 âœ“
   noise     : div shape=(2, 10), mean=-4.0000 âœ“
   target    : div shape=(2, 10), mean=4.0000 âœ“

âœ… Test 3: Multiple Hutchinson samples (reduce variance)
    1 samples: mean=0.0000, std=0.0000 âœ“
    5 samples: mean=0.0000, std=0.0000 âœ“
   10 samples: mean=0.0000, std=0.0000 âœ“

âœ… Test 4: Efficiency comparison
   10 iterations: 2.359s (235.9ms per call) âœ“
```

## Summary

| Prediction Target | Velocity Formula | Divergence Formula |
|-------------------|------------------|-------------------|
| VELOCITY | `v = crn_output` | `div(v) = trace(J_crn)` |
| NOISE | `v = (Îµ - x_t) / (1-t)` | `div(v) = (trace(J_crn) - D) / (1-t)` |
| TARGET | `v = (x_t - x_0) / t` | `div(v) = (D - trace(J_crn)) / t` |

## Files Modified

- **`src/models/mnist_flow_2d.py`**
  - Added `compute_crn_jacobian_trace()` method
  - Added `compute_velocity_divergence()` method
  - Both methods correctly account for `prediction_target`

## Implementation History

### Version 2: Hutchinson's Estimator (Current)
**Date**: December 16, 2025

- **Method**: Hutchinson's trace estimator with Rademacher sampling
- **Complexity**: O(D) per point per sample
- **Memory**: O(D) per point
- **Advantages**:
  - Standard practice in continuous normalizing flows
  - Scales to high-dimensional spaces
  - Uses `jax.jvp` for efficient Jacobian-vector products
  - Configurable variance via `hutchinson_samples` parameter

### Version 1: Full Jacobian (Deprecated)
**Date**: December 16, 2025

- **Method**: Compute full Jacobian matrix, then take trace
- **Complexity**: O(DÂ²) per point
- **Memory**: O(DÂ²) per point
- **Issues**:
  - Inefficient for high-dimensional spaces
  - Unnecessarily computes off-diagonal elements
  - Not standard practice

## Future Improvements

1. **Adaptive Sampling**: Automatically adjust `hutchinson_samples` based on variance
2. **Antithetic Sampling**: Use paired samples (v, -v) to reduce variance
3. **Stratified Sampling**: Better coverage of the random space
2. **JVP-based computation**: More memory efficient
3. **Caching**: Cache Jacobian traces during training
4. **Batch optimization**: Vectorize more efficiently

## References

- **Continuous Normalizing Flows**: Chen et al., "Neural Ordinary Differential Equations" (2018)
- **Flow Matching**: Lipman et al., "Flow Matching for Generative Modeling" (2023)
- **Hutchinson's Estimator**: Hutchinson, "A stochastic estimator of the trace of the influence matrix" (1990)

âœ… **Divergence computation correctly accounts for what the CRN predicts!** ðŸŽ¯

